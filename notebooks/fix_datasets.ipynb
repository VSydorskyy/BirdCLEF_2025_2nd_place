{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac5152-8197-4ae9-ab9a-d090a55fcee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchaudio.transforms import Resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1900e6a-d136-4968-a1c8-b100f677c2fc",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7356bd7-9f81-491e-bf7d-0e4a836bd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUDIO_FILTERS = {  # list of tuple (start, end) and/or float (time of vocalization)\n",
    "    'colcha1/XC337020': [(50, 200)],\n",
    "    'colcha1/XC532406': [(0, 8)],\n",
    "    'chbant1/XC315058': [(0, 19)],\n",
    "    '52884/CSA15755': [(9.0, 22.57), (27, 49)],\n",
    "    'gybmar/XC9608': [(0, 5), (25, 30), (35, 39)],\n",
    "    '1194042/CSA18783': [(0.9, 18.5), (20, 26)],\n",
    "    '1194042/CSA18802': [(0.9, 14.2), (25, 30)],\n",
    "    '1346504/CSA18792': [(0, 21)],\n",
    "    '134933/iNat1108984': [(1, 6), (11, 16), (21, 27)],\n",
    "    '134933/iNat1160199': [(0, 20), (29, 47)],\n",
    "    '21038/iNat297879.ogg': [(0, 12)],\n",
    "    '21038/iNat65519': [(13, 120), (160, 300)],\n",
    "    '21116/iNat65520': [(0, 6)],\n",
    "    '24272/XC882885': [(5, 33), (40, 47), (49, 56)],\n",
    "    '41778/XC959831': [(20, 35), (50, 75), (80, 123), (145, 170)],\n",
    "    '42087/iNat155127': [(5, 12)],\n",
    "    '47067/iNat68676': [(6, 43)],\n",
    "    '548639/CSA34187': [(0, 8), (5, 10)],\n",
    "    '555142/iNat31004': [(0, 8)],\n",
    "    '64862/CSA18218': [(4.5, 22), (98, 135), (154, 161), (211, 235), (270, 290)],\n",
    "    '64862/CSA18222': [(4.1, 30), (70, 95)],\n",
    "    '65547/iNat1103224': [(0, 12), (11, 16.8)],\n",
    "    '714022/CSA34203': [(0, 5.5), (2.5, 11), (8, 17), (17, 25)],\n",
    "    '714022/CSA34204': [(0,6), (4, 12), (12, 20), (19, 27), (25, 34), (30, 37)],\n",
    "    '714022/CSA34205': [(0, 5.2), (5, 14), (15.5, 24), (25, 34), (33, 40)],\n",
    "    '714022/CSA34206': [(0, 7), (6, 15), (15, 23), (22,28)],\n",
    "    '714022/CSA34207': [(0, 5.5), (7, 16), (18, 26), (29, 36), (35.8, 42)],\n",
    "    '135045/iNat1122209': [(0, 10), (12, 22), (24, 32), (36, 46)],\n",
    "    '135045/iNat1207345': [(9, 19), (34, 44), (64, 73), (69, 78), (78, 108), (105, 120), (120, 128)],\n",
    "    '135045/iNat1207347': [(4, 14), (17, 42), (42, 50), (49.5, 58), (57, 66), (67, 76), (77, 87), (90, 100), (104, 111.4)],\n",
    "    '135045/iNat1208549': [(9, 19), (27, 36), (51, 60.5), (64, 73), (78, 86.5), (93, 100), (104, 112), (120, 129), (145, 154), (169, 176.8)],\n",
    "    '135045/iNat1208550': [(0, 7.5), (11, 20), (21, 30), (33, 41), (44.5, 53), (58, 66), (71, 81), (84, 94), (94, 104), (107, 116.5), (120, 130)],\n",
    "    '135045/iNat1208551': [(6, 16), (15, 52), (55, 65), (67, 97), (101, 111), (116, 126), (128, 149), (147, 161), (160, 169), (170, 183.5), (185, 200)],\n",
    "    '135045/iNat1208552': [(0,13), (15, 74), (84, 95), (95, 116), (123, 138), (136.5, 148), (149, 159), (158, 169), (171, 183), (189, 203)],\n",
    "    '135045/iNat1208572': [(0, 30), (30, 41), (39, 51), (57, 71), (74, 84.5), (86.5, 96.5), (97, 107), (106, 117.5), (118, 136), (138, 152), (149, 198), (197, 212.2)],\n",
    "    '135045/iNat327127': [(0, 9)],\n",
    "    '135045/iNat48803': [(0, 8), (20.5, 31), (44, 51.3)],\n",
    "    'norscr1/XC146508': [0, 6, 13, 19, 26, 30, 35, 42, 48, 53, 61, 63, 64, 69, 80, 87, 99, 107],\n",
    "    'norscr1/XC148047': [2, 6, 20, 24, 28, 41, 46, 57, 65, 69, 76, 103, 108, 112, 116, (118, 136)],\n",
    "    'norscr1/XC178590': [1, 5, 9, 12, 17, 21, 28, 35, 39, 43, 48, 58, 62, 66, 70, 75, 80, 86, 91],\n",
    "    'norscr1/XC178594': [2, 5, 10, 17, 25, 31, 40, 44, 50, 55, 63, 68, 74, 80, 91, 98, 103, 108, 111],\n",
    "    'norscr1/XC178596': [(1, 51.15)],\n",
    "    'norscr1/iNat31894': [(5, 17.71)],\n",
    "    \n",
    "    # 31-Mar\n",
    "    '52884/CSA18797': [(16, 28), (26, 84), (92, 125), (125, 135), (141, 168), (560, 585)],\n",
    "    '52884/CSA18801': [(0, 250)],\n",
    "    '52884/CSA18804': [(0, 140), (560, 700), (740, 870)],\n",
    "    \n",
    "    # 2-Apr (large audios)\n",
    "    'compau/XC837459': [(0, 100), (620, 720), (1120, 1220)],   # not heard; there are pauses\n",
    "    'greegr/XC558126': [(0, 250)],   # not heard\n",
    "    'grekis/XC936081': [(0, 250)],   # not heard\n",
    "    'grekis/XC936811': [(0, 250)],   # not heard\n",
    "    'saffin/XC879442': [(0, 250)],   # not heard; there are pauses\n",
    "    'speowl1/XC525219': [(0, 100), (300,400), (600, 700)],   # not heard\n",
    "    'stbwoo2/XC709416': [(0, 100), (240, 340), (660, 760)],   # not heard\n",
    "    'yercac1/XC245490': [(0, 255)],   # not heard\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0dd0b-2dbc-456e-bbef-bb77aec12355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_audio_filters(raw_audio_filters):\n",
    "        \"\"\" finalize a dict of audio dilters by converting audio hits into audio sections (start, end),\n",
    "            merging them when applicable (if the distance to the previous hit <= 5s).\n",
    "            section for a hit is defined a (hit - BAND, hit + BAND)\n",
    "        \"\"\"\n",
    "        BAND = 4\n",
    "        \n",
    "        audio_filters = dict()\n",
    "        for id_, raw_sections in raw_audio_filters.items():\n",
    "            sections = []\n",
    "            prior_hit = -100\n",
    "            for sec in raw_sections:\n",
    "                if isinstance(sec, tuple):\n",
    "                    sections.append(sec)\n",
    "                    prior_hit = sec[1] - 4\n",
    "                else:\n",
    "                    if prior_hit + 5 >= sec:  # merge with previous section\n",
    "                        sections[-1] = (sections[-1][0], sec + BAND)\n",
    "                    else:\n",
    "                        start = max(sec - BAND, 0)\n",
    "                        end = 5 if start == 0 else sec + BAND\n",
    "                        sections.append((start, end))\n",
    "                    prior_hit = sec\n",
    "                        \n",
    "            audio_filters[id_] = sections\n",
    "        return audio_filters   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4223efa-986a-4cb1-8848-588e5447b0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech_filter:\n",
    "    \"\"\" Class that takes an audio and returns the start and end indices of the species\n",
    "        singing mostly by cutting off human speech.\n",
    "    \n",
    "        It identifies human speech using a combination of two mechanisms:\n",
    "        1) We accumulate the audio power over chunks of .1s and identify points in which it intersects\n",
    "            -50 db, which is considered silence. Usually a silent period occurs before the author adds\n",
    "            speech comments to the audio.\n",
    "        2) We use a model to identify the segments (start / end) of speech in the audio.\n",
    "        \n",
    "        We use the following parameters:\n",
    "            - sing_min_duration: min duration of singing between two periods of silence (power=-50db).\n",
    "            - speech_notes_time: if silence initiates (power=-50db) after this many seconds we cut the\n",
    "                audio (likely speech follows).\n",
    "            - spech_merge_th: two consecutive speeches with an interval smaller than `spech_merge_th`\n",
    "                are merged into a single speech\n",
    "            - speech_start_th: spech that occurs before `speech_start_th` seconds is considered a false\n",
    "                positive.\n",
    "            - speech_min_duration: any speech that last less than this number of seconds is ignored\n",
    "                \n",
    "        And implement the following rules (firt 1a/1b and then 2):\n",
    "        1a) If power crosses -50 db up and then down, for a duration >= sing_min_duration then we mark\n",
    "            those two points as the start and end of the audio.\n",
    "        1b) otherwise, if power crosses -50 down after speech_notes_time seconds, we mark the audio as\n",
    "            starting at 0 and ending at that point.\n",
    "        \n",
    "        2) We then use a model to identify periods of speech and traverse those periods:\n",
    "            - if a speech period initiates within spech_merge_th seconds of the previous one, we merge\n",
    "                them.\n",
    "            - if the duration of the current speech is >= speech_min_duration:\n",
    "                - if the speech start time <= speech_start_th, we assume it as false positive and ignore\n",
    "                    further speech periods.\n",
    "                - otherwise we cut the audio at the start of the speech and return\n",
    "            - if the duration of the current speech > .5 and speech start time > 30 seconds, we cut the \n",
    "                audio at the start of the speech and return.\n",
    "            - otherwise we move to the following speech period.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sing_min_duration=2, speech_notes_time=7, spech_merge_th=.3, speech_min_duration=2, \n",
    "                 speech_start_th=8, th=.5, threads=1, sr=32_000, speech_db_th=-50):\n",
    "        self.sing_min_duration = sing_min_duration\n",
    "        self.speech_notes_time = speech_notes_time\n",
    "        self.spech_merge_th = spech_merge_th\n",
    "        self.speech_min_duration = speech_min_duration\n",
    "        self.speech_start_th = speech_start_th\n",
    "        self.th = th\n",
    "        self.sr = sr\n",
    "        self.speech_db_th = speech_db_th\n",
    "        \n",
    "        # interval variables\n",
    "        self.chunk_len = 0.1\n",
    "        self.chunk = int(self.chunk_len * self.sr)\n",
    "        torch.set_num_threads(threads)\n",
    "        self.model, (self.get_speech_timestamps, _, read_audio, _, _) = \\\n",
    "            torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "\n",
    "            \n",
    "    def __call__(self, audio, sr, th=None):\n",
    "        assert sr == self.sr\n",
    "        \n",
    "        if len(audio.shape) > 1:\n",
    "            audio = audio[0]\n",
    "        len_ = len(audio)\n",
    "        \n",
    "        # power based detection\n",
    "        chunk = int(self.chunk_len * self.sr)\n",
    "        power = audio ** 2\n",
    "        pad = int(np.ceil(len(power) / chunk) * chunk - len(power))\n",
    "        power = np.pad(power, (0, pad))\n",
    "        power = power.reshape((-1, chunk)).sum(axis=1)\n",
    "        power_dB = 10 * np.log10(power)\n",
    "        x = power_dB - self.speech_db_th\n",
    "        start, end = 0, len_\n",
    "        intersections = np.where(x[:-1] * x[1:] < 0)[0]\n",
    "        for s, e in zip(intersections[:-1], intersections[1:]):\n",
    "            if x[s] < x[s+1] and (e - s) * self.chunk_len >= self.sing_min_duration:\n",
    "                start, end = s * chunk, e * chunk\n",
    "                break\n",
    "            elif x[s] > x[s+1] and s * self.chunk_len > self.speech_notes_time:\n",
    "                start, end = 0, s * chunk\n",
    "                break\n",
    "\n",
    "        # model based detection\n",
    "        threshold = th if th is not None else self.th\n",
    "        speech_timestamps = self.get_speech_timestamps(\n",
    "            audio[start : end], self.model, sampling_rate=self.sr, threshold=threshold)\n",
    "        \n",
    "        if len(speech_timestamps) > 0:\n",
    "            s, e = -1e6, -1e6\n",
    "            # print(ts_to_s(speech_timestamps))\n",
    "            for ts in speech_timestamps:\n",
    "                if ts['start'] - e < self.spech_merge_th * self.sr:  # merge\n",
    "                    e = ts['end']\n",
    "                else:\n",
    "                    s, e = ts['start'], ts['end']\n",
    "                duration = (e - s) / self.sr\n",
    "                start_s = (start + s) / self.sr\n",
    "                if duration >= self.speech_min_duration or (duration > .5 and start_s >= 30):\n",
    "                    if start_s <= self.speech_start_th:\n",
    "                        break  # probably a false positive\n",
    "                    start, end = start, start + s\n",
    "                    break\n",
    "        return start, end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdd9e60-acc8-478f-9d6c-8df5c3f03662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_curation_pipeline(\n",
    "    filenames, \n",
    "    custom_filter={}, \n",
    "    speech_filter=None,\n",
    "    required_sr=32_000\n",
    "):\n",
    "    \n",
    "    files_edges = dict()\n",
    "    for filename in tqdm(filenames):\n",
    "        audio, sr = torchaudio.load(filename)\n",
    "        if sr != required_sr:\n",
    "            resampler = Resample(\n",
    "                orig_freq=sr, new_freq=required_sr\n",
    "            )\n",
    "            sr = required_sr\n",
    "            audio = resampler(audio)\n",
    "        audio = audio[0]\n",
    "        id_ = os.path.splitext(filename)[0]\n",
    "        id_ = \"/\".join(id_.split(\"/\")[-2:])\n",
    "        sections = custom_filter.get(id_, None)\n",
    "        if sections is not None:\n",
    "            start = int(sections[0][0] * sr)\n",
    "            end = min(int(sections[-1][1] * sr), len(audio))\n",
    "        elif speech_filter is not None:\n",
    "            start, end = speech_filter(audio, sr)\n",
    "            end = min(end, len(audio))            \n",
    "                \n",
    "        files_edges[id_] = (start, end)\n",
    "    \n",
    "    return files_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef6ef5-598c-4568-9aba-7c02701261f7",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace0d65-10b0-4889-bcc4-712c8a2dfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_filters = complete_audio_filters(TRAIN_AUDIO_FILTERS)\n",
    "speech_filter = Speech_filter(threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30efc56-64d2-4081-baaa-2eff3cd161df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = (\n",
    "    glob(\"../data/train_audio/*/*.*\") +\n",
    "    glob(\"../data/add_train_audio_from_prev_comps/*/*.*\") +\n",
    "    glob(\"../data/add_train_audio_from_xeno_canto_28032025/*/*.*\") \n",
    ")\n",
    "len(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77baad95-8931-4308-ad75-52b9e9afb169",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames_bounds = run_curation_pipeline(\n",
    "    all_filenames,\n",
    "    custom_filter=train_audio_filters,\n",
    "    speech_filter=speech_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01629323-3d8b-46fd-b4c0-9969d089bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5cd47-b022-48a9-8080-b0540d834a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382d85f-9751-44d9-af98-9f2411487597",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7502642-721f-4e16-a063-6fbe3382e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"../data/train_audio/41778/XC959831.ogg\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
